%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Template for a conference paper, prepared for the
%% Food and Resource Economics Department - IFAS
%% UNIVERSITY OF FLORIDA
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Version 1.0 // November 2019
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Ariel Soto-Caro
%%  - asotocaro@ufl.edu
%%  - arielsotocaro@gmail.com
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt]{article}
\usepackage{UF_FRED_paper_style}

\usepackage{lipsum}  %% Package to create dummy text (comment or erase before start)
\usepackage{media9}
\usepackage{multimedia}

%% ===============================================
%% Setting the line spacing (3 options: only pick one)
% \doublespacing
% \singlespacing
\onehalfspacing
%% ===============================================

\setlength{\droptitle}{-5em} %% Don't touch

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SET THE TITLE
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TITLE:
\title{Reinforcement learning approach for finding adversarial examples

}

% AUTHORS:
\author{Gilad Ben Or\\% Name author
    \href{mailto:benorgi@post.bgu.ac.il}{\texttt{benorgi@post.bgu.ac.il}} %% Email author 1 
}
    
% DATE:
\date{\today}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{\setstretch{.8}
\maketitle
% %%%%%%%%%%%%%%%%%%
\begin{abstract}
% CONTENT OF ABS HERE--------------------------------------
Dummy text for abstract. 

% END CONTENT ABS------------------------------------------
\noindent
\textit{\textbf{Keywords: }%
key1; key2; key3; key4.} \\ %% <-- Keywords HERE!
\noindent
\textit{\textbf{JEL Classification: }%
Q12; C22; D81.} %% <-- JEL code HERE!

\end{abstract}
}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BODY OF THE DOCUMENT
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% --------------------
\section{Introduction}
% --------------------
Many high-performance machine learning algorithms are susceptible to minimal changes in their inputs. Adversarial perturbations drawn attention from two directions. On the one hand, we are concerned about the integrity and security of machine learning algorithms deployed, such as autonomous cars or face-recognition systems. On the other hand, it is a exciting research opportunity to understand the difference between human and computer sensory information processing.

Decision-based attacks are a category of black box attacks which depend solely on the model's final decision. In terms of the structure of the inputs and outputs and certain examples classified by the model, the attacker has little knowledge regarding the  the model to be attacked.

Previous works suggested algorithms for finding adversarial examples with minimal perturbation size and minimal calls of the model to be attacked. \cite{brendel2017decision} introduced a the boundary attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. \cite{chen2019hopskipjumpattack} proposed a novel unbiased estimate of gradient direction at the decision boundary based solely on access to model decisions. Their algorithms requires significantly fewer model queries than several state-of-the-art decision-based adversarial attacks and achieves competitive performance in attacking several widely-used defense mechanisms.

In order to design and evaluate more attack algorithms, there is a real need for an environment that allows data interaction and results measurement. The environment is similar to, and can be reduced to, reinforcement learning environment. reinforcement learning environment offers the opportunity to observe data , perform a single or sequence of actions and receive the corresponding score. Here, the attack algorithms are equivalent to the reinforcement learning policies.

The use of such an environment will improve the implementation of new algorithms and will be a standard tool for testing them. Thus, researchers may be able to work with state-of-the-art algorithms and methodologies, reinforcement learning in particular.

In this work, I developed a decision-based attack environment based on the OpenAI Reinforcement Learning Gym \cite{brockman2016openai} interface. The system fits for the MNIST dataset \cite{mnist10027939599} and can quickly be expanded to support other datasets. I have researched and added support for custom actions, implemented two policies and trained a reinforcement learning agent to discover the optimal policy.

The contributions of this work are as follows:
\begin{itemize}
\item Introduce a research framework for decision-based attacks. 
\item Suggests custom actions for attack algorithms.
\item Design, implementation and evaluation of two boundary attack algorithms.
\item Training and evaluation of reinforcement learning agent in this environment.
\end{itemize}


% --------------------
\section{Methodology}
% --------------------
\subsection{Reinforcement learning model}
The purpose of reinforcement learning model is to teach an agent to take actions in an environment in order to maximize the cumulative reward. In this work, the agent learns how to take actions in order to generate an adversarial example for a target class while maximize the rewards which is relative to its success in generating an adversarial example with minimal perturbation size.

Figure \ref{fig:rl_model} shows a widely used reinforcement learning framework. The agent accesses the environment, triggers it with an action, collects the reward and observes the new state of the environment.

In the following subsections we describe each component in the figure.

\begin{figure}[H]
    \centering
        \includegraphics[scale=.8]{figures/reinforcement-learning-fig1-700.jpg}
    \caption{Reinforcement learning framework.}
    \label{fig:rl_model}
\end{figure}

\textbf{Note:} We will have to train a different agent for each target label. \\



\subsection{Action space}
The following actions are available for the agent:
\subsubsection{DECREASE STEP}
Decrease the step size according to the equation below.: \\
step=\begin{Bmatrix}
step size > 0.4 : step size/2\\ 
o.w.: max(0.05, step_size- 0.05)
\end{Bmatrix}

\subsubsection{Increase step}
Increase the step size according to the equation below.: \\
step=\begin{Bmatrix}
step size > 0.4 : step size/2\\ 
o.w.: max(0.05, step_size- 0.05)
\end{Bmatrix}

\subsubsection{Closet cluster}
Go one step in the direction of the nearest target cluster.

\subsubsection{Farthest cluster} 
Go one step in the direction of the most distant target cluster.

\subsubsection{Original image}
Go one step in the direction of the original sample.





\subsubsection{Perturbation from normal distribution}
We adopted the iterative perturbation method describes in \cite{brendel2017decision}. This action add a perturbation from normal distribution subject to the constraints in figure \ref{fig:normal_dist_cons}
\begin{figure}[H]
    \centering
        \includegraphics[scale=.8]{figures/constraints.jpg}
    \caption{Constraints for perturbation from normal distribution}
    \label{fig:normal_dist_cons}
\end{figure}

\subsubsection{Kmeans direction}
In offline, we clustered the target label dataset to k clusters using kmeans algorithm.
Here, the agent goes small step in the direction of closet cluster. \\
option: if the perturbation is bigger than epsilon, discard the action.

\subsubsection{comments}
\begin{enumerate}
\item We have to think about more actions.
\item Maybe we want to multiply the previous actions by 2: small steps and big steps. E.g. Kmeans direction with small step and Kmeans direction with big step.
\end{enumerate}


\subsection{reward}
The reward has to take into account two factors:
\begin{enumerate}
\item The classification of the adversarial example - whether the classifier labeles the examples as the target class.
C=\begin{Bmatrix}
1: x\in targetclass\\ 
0: x\notin targetclass
\end{Bmatrix}
\item Perturbation size - the agent gets negative reward if the perturbation is bigger than maximum perturbation P and positive reward if it under this limit.
\end{enumerate}

A suggestion for the reward function is shown in (Equation \ref{eq1}):
\begin{equation} \label{eq1}
reward = \alpha C + \beta  \left (P-d\left ( o, t \right )  \right )
\end{equation}
\textbf{Ziv, we need to discuss the reward function. Since the agent tries to maxmize the cumulative reward, we should return 0 for all steps in spite the termination step. It is not a good practice for reinforcement learning. could we think about a better reward function?  }

\subsection{Is done signal}
The is done signal stops the agent while it does the exploration-exploitation tasks. We would stop the agent if it perform more than L steps, or it reached the target class under the perturbation size constraint.


\subsection{The environment}
The environment implements the classical OpenAI gym's API. The API includes the following functions:
\begin{enumerate}
\item reset() - reset the environment to initial state, return first observation. In our case, the environment returns a random item from the training set.
\item step(a) - commit action a and return the tuple (new observation, reward, is done).
\begin{itemize}
\item new observation - an observation right after committing the action a
\item reward - a number representing your reward for committing action a
\item is done - True if the MDP has just finished, False if still in progress
\end{itemize}

\end{enumerate}

\subsection{The agent}
The agent will learn his policy using one of the common reinforcement learning methods such as SARSA, DQN and etc.

\subsection{Evaluation}
\subsubsection{dataset}
MNIST

\subsubsection{The attacked classifier}
Black box convolution network based classifier.

\subsubsection{metrics}
We will measure the number of classifier calls for a generation of an adversarial example.

systemitic vs randomizsation

Because in the final layer you are approximating a real state action value for each action. You output to a linear layer as your Q value estimate can generally take on any real value. And then you add a mean squares error loss with the linear layer output.

This set up is similar any general regression problem with a neural nnetwork.

Relu is used quite often but for hidden layer activation.

Softmax output is quite popular for representing the policy network/function for discrete actions. But for Q learning, we are learning an approximate estimate of the Q value for each state action pair, which is a regression problem.

    # Select a policy. We use eps-greedy action selection, which means that a random action is selected with probability eps. We anneal eps from 1.0 to 0.1 over the course of 1M steps. This is done so that the agent initially explores the environment (high eps) and then gradually sticks to what it knows (low eps). We also set a dedicated eps value that is used during testing. Note that we set it to 0.05 so that the agent still performs some random actions. This ensures that the agent cannot get stuck.


def start_policy(obs):
    return ACTIONS.index("CLOSET_CLUSTER")

Hyperparam
Transferability 
Many calls
Dataset 
Yaron

% --------------------
\section{Results}
% --------------------

\begin{table}[]
\caption{Simple centers algorithm. The table explains the details about the minimal perturbations that happened in the episodes. Min perturbations is the minimum perturbations reached in all episodes. Max perturbations is the maximum perturbations over all the episodes' perturbation.}
\label{tab:simple_result}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{}       & \textbf{}     & \multicolumn{4}{c|}{\textbf{pertrubation}}                      & \multicolumn{4}{c|}{\textbf{steps}}                             \\ \hline
\textbf{target} & \textbf{nobs} & \textbf{min} & \textbf{max} & \textbf{mean} & \textbf{variance} & \textbf{min} & \textbf{max} & \textbf{mean} & \textbf{variance} \\ \hline
0               & 100           & 3.95         & 7.94         & 5.49          & 0.85              & 3            & 7            & 206.62        & 83467.01          \\ \hline
1               & 100           & 1.98         & 9.63         & 5.92          & 2.08              & 1            & 9            & 36.65         & 19376.63          \\ \hline
2               & 100           & 3.72         & 7.93         & 5.33          & 0.96              & 3            & 7            & 30.95         & 11255.64          \\ \hline
3               & 100           & 3.51         & 9.73         & 5.81          & 2.61              & 3            & 9            & 213.43        & 105655.64         \\ \hline
4               & 100           & 3.09         & 8.9          & 5.78          & 1.45              & 3            & 8            & 97.73         & 49380.1           \\ \hline
5               & 100           & 2.0          & 9.87         & 6.17          & 1.94              & 2            & 9            & 4.81          & 1.41              \\ \hline
6               & 100           & 2.0          & 8.8          & 6.09          & 1.44              & 2            & 8            & 204.72        & 92089.25          \\ \hline
7               & 100           & 2.0          & 9.09         & 6.14          & 1.61              & 2            & 9            & 166.11        & 88095.33          \\ \hline
8               & 100           & 2.0          & 8.42         & 4.95          & 1.37              & 2            & 8            & 71.84         & 30416.3           \\ \hline
9               & 100           & 2.0          & 7.97         & 5.3           & 1.32              & 2            & 7            & 67.57         & 28757.86          \\ \hline
\end{tabular}
\end{table}


200000 - 2383(40min) - 0000 - DECREASE_STEP
200000 - 2383(40min) - 0000 - CLOSET CLUSTER

% \includemedia[width=0.6\linewidth,height=0.6\linewidth,activate=pageopen,
% passcontext,
% transparent,
% addresource=myclip.mp4,
% flashvars={source=myclip.mp4}
% ]{\includegraphics[width=0.6\linewidth]{penguins}}{VPlayer.swf}

\begin{frame}
\frametitle{Forward Kinematics}

\begin{center}
\includemedia[
    activate=onclick,
    width=0.75\textwidth
]{\includegraphics{figures/reinforcement-learning-fig1-700.jpg}}{myclip.mp4}
\end{center}
\end{frame}
Dummy text.

% --------------------
\section{Discussion and Conclusions}
% --------------------

Dummy text. 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFERENCES SECTION
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\medskip

\bibliography{references.bib} 

\newpage

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TABLES
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{table}[H]
%   \centering
%   \caption{Example table of descriptive statistics of the main variables.}
%   \label{tab:1}
%   \scalebox{.8}{
%     \begin{tabular}{rlrrrrrr}
%     \hline
%     \multicolumn{1}{c}{\textbf{Variables}} & \multicolumn{1}{c}{\textbf{Categories}} & \multicolumn{1}{c}{\textbf{Unit}} & \multicolumn{1}{c}{\textbf{Rep}} & \multicolumn{1}{c}{\textbf{Mean}} & \multicolumn{1}{c}{\textbf{St. Dev.}} & \multicolumn{1}{c}{\textbf{Min}} & \multicolumn{1}{c}{\textbf{Max}} \\ \hline \hline

%     \multicolumn{1}{l}{Variable 1} & Category A & \multicolumn{1}{c}{\$} & \multicolumn{1}{c}{8} & 0     & 0     & 0     & 0 \\
%           & Category B & \multicolumn{1}{c}{lb} & \multicolumn{1}{c}{8} & 22,411.20 & 6,325.90 & 13,819 & 31,201 \\
%           & Category C & \multicolumn{1}{c}{\$} & \multicolumn{1}{c}{8} & 5,869.60 & 4,609.90 & -464.1 & 12,744.10 \\
%     \multicolumn{1}{l}{Variable 2} & Category A & \multicolumn{1}{c}{\$} & \multicolumn{1}{c}{8} & 1,777.40 & 144.5 & 1,642.30 & 1,912.60 \\
%           & Category B & \multicolumn{1}{c}{lb} & \multicolumn{1}{c}{8} & 21,444.80 & 5,146.90 & 15,096 & 28,032 \\
%           & Category C & \multicolumn{1}{c}{\$} & \multicolumn{1}{c}{8} & 4,138.50 & 2,644.10 & 22.2  & 7,932.70 \\
%     \multicolumn{1}{l}{Variable 3} & Category A & \multicolumn{1}{c}{\$} & \multicolumn{1}{c}{8} & 2,346.80 & 190.8 & 2,168.30 & 2,525.20 \\
%           & Category B & \multicolumn{1}{c}{lb} & \multicolumn{1}{c}{8} & 18,343.30 & 2,460.70 & 15,269.00 & 21,524.10 \\
%           & Category C & \multicolumn{1}{c}{\$} & \multicolumn{1}{c}{8} & 3,699.20 & 2,549.80 & 1,299.10 & 8,709.80 \\
%     \multicolumn{1}{l}{Variable 4} & Category A & \multicolumn{1}{c}{\$} & \multicolumn{1}{c}{8} & 2,288.80 & 186.1 & 2,114.80 & 2,462.90 \\
%           & Category B & \multicolumn{1}{c}{lb} & \multicolumn{1}{c}{8} & 23,450.40 & 4,172.50 & 20,045.00 & 32,363.00 \\
%           & Category C & \multicolumn{1}{c}{\$} & \multicolumn{1}{c}{8} & 6,619.80 & 1,918.40 & 4,479.70 & 10,633.90 \\
% \hline
%           & CASE \#1 &       &       & 14    & 6.61  & 6.9   & 27.9 \\
%           & CASE \#2 &       &       & 22.8  & 7.73  & 10.2  & 31.4 \\
%     \hline
%     \end{tabular}}
% \end{table}%


% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % FIGURES
% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[H]
%     \centering
%         \includegraphics[scale=.6]{figures/example_figure.png}
%     \caption{Example figure.}
%     \label{fig:1}
% \end{figure}

% ==========================
% ==========================
% ==========================


\end{document}